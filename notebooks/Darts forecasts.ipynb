{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df973d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install watermark \"u8darts[torch]\" lightgbm plotly cufflinks numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcd38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U \"u8darts[torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db094daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+git://github.com/unit8co/darts.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37205ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51508db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%reload_ext watermark\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c05593",
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b31a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge 'u8darts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e47341",
   "metadata": {},
   "source": [
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9546aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import darts\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.offline\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a9ba7",
   "metadata": {},
   "source": [
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea8e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd84c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv(\"template.csv\", index_col=0)\n",
    "df_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m6 = pd.read_csv(\"M6_Universe.csv\", index_col=0)\n",
    "df_m6.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4444fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m6_stocks = df_m6[df_m6[\"class\"]==\"Stock\"]\n",
    "df_m6_etf = df_m6[df_m6[\"class\"]==\"ETF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91baa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805355e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import get_ticker_historical_data\n",
    "\n",
    "directory = './tickers'\n",
    "save = False\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "tickers = df_m6[\"symbol\"].to_list()\n",
    "tickers_data = dict()\n",
    "from_date = pd.to_datetime(\"2000-01-01\")\n",
    "\n",
    "to_date = pd.Timestamp.today()\n",
    "to_date.tz_localize(tz='Europe/Moscow').tz_convert(tz='America/New_York')\n",
    "to_date.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# to_date = pd.to_datetime(\"2022-01-30\")\n",
    "interval = '1d'\n",
    "\n",
    "for ticker in tqdm(tickers[:SAMPLE_SIZE]): \n",
    "    data = get_ticker_historical_data(ticker=ticker,\n",
    "                                      from_date=from_date,\n",
    "                                      to_date=to_date,\n",
    "                                      interval=interval\n",
    "                                      )\n",
    "    tickers_data[ticker] = data\n",
    "    if save:\n",
    "        data.reset_index().to_csv(os.path.join(directory,f'{ticker}_{interval}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pct_returns(x: pd.Series) -> pd.Series:\n",
    "    return (1 + x.pct_change(periods=1))\n",
    "\n",
    "def calculate_cum_pct_returns(x: pd.Series) -> pd.Series:\n",
    "    return (((1 + x.pct_change(periods=1)).cumprod() - 1))*100\n",
    "\n",
    "def calculate_cum_log_returns(x: pd.Series) -> pd.Series:\n",
    "    return (np.log(1 + x.pct_change(periods=1)).cumsum())\n",
    "\n",
    "def calculate_log_returns(x: pd.Series) -> pd.Series:\n",
    "    return np.log(1 + x.pct_change(periods=1))\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict({k: v['Adj Close'] for k, v in tickers_data.items()})\n",
    "df_stock_cum_log_returns = df.apply(calculate_cum_log_returns, axis=0)\n",
    "df_stock_cum_prt_returns = df.apply(calculate_cum_pct_returns, axis=0)\n",
    "df_stock_log_returns = df.apply(calculate_log_returns, axis=0)\n",
    "df_stock_prc_returns = df.apply(calculate_pct_returns, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b23508",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_returns = df_stock_cum_log_returns.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9252e6",
   "metadata": {},
   "source": [
    "### Reindex dates and fill in with previous values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e1ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.time_feature.holiday import (\n",
    "    squared_exponential_kernel,\n",
    "    SpecialDateFeatureSet,\n",
    "    NEW_YEARS_DAY,\n",
    "    MARTIN_LUTHER_KING_DAY,\n",
    "    PRESIDENTS_DAY,\n",
    "    GOOD_FRIDAY,\n",
    "    MEMORIAL_DAY,\n",
    "    INDEPENDENCE_DAY,\n",
    "    LABOR_DAY,\n",
    "    THANKSGIVING,\n",
    "    CHRISTMAS_DAY,\n",
    "    SUPERBOWL,\n",
    "    CHRISTMAS_EVE,\n",
    "    EASTER_SUNDAY,\n",
    "    EASTER_MONDAY,\n",
    "    MOTHERS_DAY,\n",
    "    COLUMBUS_DAY,\n",
    "    NEW_YEARS_EVE,\n",
    "    BLACK_FRIDAY,\n",
    "    CYBER_MONDAY\n",
    ")\n",
    "\n",
    "# Example use for using a squared exponential kernel:\n",
    "kernel = squared_exponential_kernel(alpha=1.0)\n",
    "sfs = SpecialDateFeatureSet([NEW_YEARS_DAY,\n",
    "                             MARTIN_LUTHER_KING_DAY,\n",
    "                             PRESIDENTS_DAY,\n",
    "                             GOOD_FRIDAY,\n",
    "                             MEMORIAL_DAY,\n",
    "                             INDEPENDENCE_DAY,\n",
    "                             LABOR_DAY,\n",
    "                             THANKSGIVING,\n",
    "                             CHRISTMAS_DAY],\n",
    "                            kernel)\n",
    "\n",
    "sfs2 = SpecialDateFeatureSet([SUPERBOWL,\n",
    "                              CHRISTMAS_EVE,\n",
    "                              EASTER_SUNDAY,\n",
    "                              EASTER_MONDAY,\n",
    "                              MOTHERS_DAY,\n",
    "                              COLUMBUS_DAY,\n",
    "                              NEW_YEARS_EVE,\n",
    "                              BLACK_FRIDAY,\n",
    "                              CYBER_MONDAY],\n",
    "                            kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_ta as ta\n",
    "\n",
    "# Create our own Custom Strategy\n",
    "CustomStrategy = ta.Strategy(\n",
    "    name=\"Momo and Volatility\",\n",
    "    description=\"SMA 50,200, BBANDS, RSI, MACD and Volume SMA 20\",\n",
    "    ta=[\n",
    "        {\"kind\": \"sma\", \"length\": 20, \"close\": \"Adj Close\"},\n",
    "        {\"kind\": \"sma\", \"length\": 5, \"close\": \"Adj Close\"},\n",
    "        #{\"kind\": \"sma\", \"length\": 200, \"close\": \"Adj Close\"},\n",
    "        {\"kind\": \"ema\", \"length\": 8, \"close\": \"Adj Close\"},\n",
    "        {\"kind\": \"ema\", \"length\": 21, \"close\": \"Adj Close\"},\n",
    "#         {\"kind\": \"ema\", \"length\": 50, \"close\": \"Adj Close\"},\n",
    "        {\"kind\": \"bbands\", \"length\": 20, \"close\": \"Adj Close\"},\n",
    "        {\"kind\": \"rsi\", \"length\": 14, \"close\": \"Adj Close\"},\n",
    "        {\"kind\": \"stochrsi\", \"length\": 14, \"close\": \"Adj Close\"},\n",
    "        {\"kind\": \"macd\", \"fast\": 8, \"slow\": 21, \"close\": \"Adj Close\"},\n",
    "        {\"kind\": \"stoch\", \"fast\": 9, \"slow\": 6, \"close\": \"Adj Close\"},\n",
    "        {\"kind\": \"macd\", \"fast\": 12, \"slow\": 26, \"close\": \"Adj Close\"},\n",
    "        {\"kind\": \"sma\", \"close\": \"Volume\", \"length\": 20, \"prefix\": \"Volume\"},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960eb4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pipeline with the steps\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from transformers import DateTimeTransformer, periodic_spline_transformer\n",
    "from reduce_memory import ReduceMemoryTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "date_time_transforms = make_pipeline(\n",
    "    DateTimeTransformer()\n",
    ")\n",
    "\n",
    "memory_transforms = make_pipeline(\n",
    "    ReduceMemoryTransformer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nyse_holidays import NYSECalendar\n",
    "\n",
    "def get_datetime_covariates(start_index, end_index):\n",
    "    calendar = NYSECalendar()\n",
    "    index = pd.date_range(start=start_index, end=end_index, freq='D')\n",
    "    holiday_dates = calendar.holidays(start_index, end_index, return_name=True).index\n",
    "    covariates = pd.DataFrame(index=index)\n",
    "    covariates.loc[:, ['one_hot_weekends', 'one_hot_holidays']] = 0\n",
    "    covariates.loc[covariates.index.isin(holiday_dates), 'one_hot_holidays'] = 1 \n",
    "    covariates.loc[covariates.index.day_name().isin(['Saturday', 'Sunday']),'one_hot_weekends'] = 1\n",
    "    covariates.loc[:,'kernel_holidays'] = sfs(covariates.index).max(axis=0) # np.prod(sfs(covariates.index), axis=1)\n",
    "    covariates.loc[:,'kernel_other_holidays'] = sfs2(covariates.index).max(axis=0)\n",
    "    covariates = covariates.round(3)\n",
    "\n",
    "    covariates = date_time_transforms.fit_transform(covariates)\n",
    "    month_splines = periodic_spline_transformer(12, n_splines=6).fit_transform(covariates[['month']])\n",
    "    weekday_splines = periodic_spline_transformer(7, n_splines=3).fit_transform(covariates[['day_of_week']])\n",
    "    splines = np.concatenate((month_splines, weekday_splines), axis=1)\n",
    "    spline_names = [f\"spline_{i}\" for i in range(splines.shape[1])]\n",
    "    covariates.loc[:, spline_names] = splines\n",
    "    covariates = memory_transforms.fit_transform(covariates)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    covariates = pd.DataFrame(data=scaler.fit_transform(covariates), \n",
    "                              index=covariates.index, \n",
    "                              columns=covariates.columns)\n",
    "\n",
    "    return covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8f1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = df_stock_returns.index[0]\n",
    "end_index = df_stock_returns.index[-1]\n",
    "df_stock_returns = (df_stock_returns\n",
    "        .reindex(pd.date_range(start=start_index, end=end_index, freq='D'))\n",
    "        .fillna(method='ffill')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8936247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = get_datetime_covariates(start_index, end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77595b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_data_enriched = {}\n",
    "\n",
    "for k, v in tickers_data.items():\n",
    "    df = v.copy()\n",
    "    df.ta.strategy(CustomStrategy)\n",
    "    df.ta.percent_return(cumulative=False, append=True)\n",
    "    df = (df\n",
    "        .reindex(pd.date_range(start=df.index[0], end=df.index[-1], freq='D'))\n",
    "        .fillna(method='ffill')\n",
    "        .fillna(method='bfill')\n",
    "    )\n",
    "    \n",
    "    df = memory_transforms.fit_transform(df)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = pd.DataFrame(data=scaler.fit_transform(df), \n",
    "                             index=df.index, \n",
    "                             columns=df.columns)\n",
    "    df_scaled.dropna(inplace=True)\n",
    "    tickers_data_enriched[k] = df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fd2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import (\n",
    "    Scaler,\n",
    "    MissingValuesFiller,\n",
    "    Mapper,\n",
    "    InvertibleMapper,\n",
    ")\n",
    "scaled_series = list()\n",
    "future_covariates = list()\n",
    "past_covariates = list()\n",
    "scalers = list()\n",
    "\n",
    "for column in tqdm(df_stock_returns.columns): \n",
    "    df = df_stock_returns[[column]].copy()\n",
    "    scaler = Scaler()\n",
    "    filler = MissingValuesFiller()\n",
    "    \n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    future_cov = covariates.copy()\n",
    "    future_cov = future_cov.loc[df.index[0]:df.index[-1],:]\n",
    "    \n",
    "    past_cov = tickers_data_enriched[column].copy()\n",
    "    past_cov = past_cov.loc[df.index[0]:df.index[-1],:]\n",
    "\n",
    "    serie = TimeSeries.from_dataframe(df.reset_index(), \n",
    "                                      time_col='index',\n",
    "                                      fill_missing_dates=False,\n",
    "                                      freq='D'\n",
    "                                     )\n",
    "    scaled_serie = scaler.fit_transform(serie)\n",
    "    filled = filler.transform(scaled_serie, method=\"quadratic\")\n",
    "    \n",
    "    past_cov_series = TimeSeries.from_dataframe(past_cov.reset_index(), \n",
    "                                                time_col='index',\n",
    "                                                fill_missing_dates=False,\n",
    "                                                freq='D'\n",
    "                                                )\n",
    "    future_cov_series = TimeSeries.from_dataframe(future_cov.reset_index(), \n",
    "                                                  time_col='index',\n",
    "                                                  fill_missing_dates=False,\n",
    "                                                  freq='D'\n",
    "                                                 )\n",
    "    \n",
    "    scalers.append(scaler)\n",
    "    scaled_series.append(filled)\n",
    "    future_covariates.append(future_cov_series)\n",
    "    past_covariates.append(past_cov_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7fb916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.utils.statistics import plot_acf, check_seasonality\n",
    "\n",
    "for serie in scaled_series[:1]:\n",
    "    plot_acf(serie, m=125, alpha=0.05, max_lag=540)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1558f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for serie in scaled_series:\n",
    "    for m in range(2, 25):\n",
    "        is_seasonal, period = check_seasonality(serie, m=m, alpha=0.05)\n",
    "        if is_seasonal:\n",
    "            print(\"There is seasonality of order {}.\".format(period))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "[serie.plot() for serie in scaled_series[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92715004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_series[0].pd_dataframe()\n",
    "# scaled_series[0].values()\n",
    "# scaled_series[0].all_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3176f14d",
   "metadata": {},
   "source": [
    "### Future covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411404cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from darts import concatenate\n",
    "# from darts.utils.timeseries_generation import datetime_attribute_timeseries as dt_attr\n",
    "# from darts.utils.timeseries_generation import holidays_timeseries as holiday_attr\n",
    "# from darts.utils.timeseries_generation import linear_timeseries\n",
    "\n",
    "# future_covs = [concatenate(\n",
    "#                         [\n",
    "#                             dt_attr(series.time_index, \"month\", one_hot=True, dtype=np.float32),\n",
    "#                             #dt_attr(series.time_index, \"month\", cyclic=True, dtype=np.float32),\n",
    "#                             dt_attr(series.time_index, \"week\", one_hot=True, dtype=np.float32),\n",
    "#                             #dt_attr(series.time_index, \"week\", cyclic=True, dtype=np.float32),\n",
    "#                             dt_attr(series.time_index, \"weekday\", one_hot=True, dtype=np.float32),\n",
    "#                             #dt_attr(series.time_index, \"weekday\", cyclic=True, dtype=np.float32),\n",
    "#                             dt_attr(series.time_index, \"day\", one_hot=True, dtype=np.float32),\n",
    "#                             #dt_attr(series.time_index, \"day\", cyclic=True, dtype=np.float32),\n",
    "#                             (dt_attr(series.time_index, \"year\", dtype=np.float32) - 2000) / 12,\n",
    "#                             holiday_attr(series.time_index, country_code=\"US\", dtype=np.float32),\n",
    "#                             linear_timeseries(start=series.time_index[0], end=series.time_index[-1], dtype=np.float32)\n",
    "#                         ],\n",
    "#                             axis=\"component\",\n",
    "#                         ) for series in scaled_series]\n",
    "\n",
    "# future_covs = [concatenate(\n",
    "#                         [\n",
    "#                             dt_attr(series.time_index, \"month\", cyclic=True, dtype=np.float32),\n",
    "#                             dt_attr(series.time_index, \"week\", cyclic=True, dtype=np.float32),\n",
    "#                             dt_attr(series.time_index, \"weekday\", cyclic=True, dtype=np.float32),\n",
    "#                             dt_attr(series.time_index, \"day_of_week\", cyclic=True, dtype=np.float32),\n",
    "#                             (dt_attr(series.time_index, \"year\", dtype=np.float32) - 2000) / 12,\n",
    "#                             holiday_attr(series.time_index, country_code=\"US\", dtype=np.float32),\n",
    "#                             linear_timeseries(start=series.time_index[0], end=series.time_index[-1], dtype=np.float32)\n",
    "#                         ],\n",
    "#                             axis=\"component\",\n",
    "#                         ) for series in scaled_series]\n",
    "\n",
    "# future_covs.plot()\n",
    "# plt.title(\n",
    "#     \"one multivariate time series of 2 dimensions, containing covariates for the air series:\"\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411c230",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizon = 19\n",
    "start_split = 0.9\n",
    "input_chunk_length = 38\n",
    "quantiles = [0.05, 0.5, 0.95]\n",
    "\n",
    "splited_series = [serie.split_before(start_split) for serie in scaled_series] # if serie.n_timesteps > 500]\n",
    "splited_past_covariates = [serie.split_before(start_split) for serie in past_covariates]\n",
    "splited_future_covariates = [serie.split_before(start_split) for serie in future_covariates]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f52891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from utils import print_error_metrics\n",
    "from darts.metrics import mape, r2_score, mse, rmse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def backtest_local_models(models, \n",
    "                          scaled_series, \n",
    "                          past_covariates, \n",
    "                          future_covariates,\n",
    "                          forecast_horizon, \n",
    "                          start_split,\n",
    "                          verbose=False) -> List:\n",
    "    backtests = [model.historical_forecasts(series=serie,\n",
    "                                            past_covariates=past_cov,\n",
    "                                            future_covariates=future_cov,\n",
    "                                            forecast_horizon=forecast_horizon,\n",
    "                                            start=start_split,\n",
    "                                            stride=1,\n",
    "                                            retrain=False,\n",
    "                                            last_points_only=True,\n",
    "                                            verbose=verbose)\n",
    "                 for model, serie, past_cov, future_cov in list(zip(models, \n",
    "                                                                    scaled_series, \n",
    "                                                                    past_covariates, \n",
    "                                                                    future_covariates))]\n",
    "    return backtests\n",
    "\n",
    "def backtest_global_model(model, \n",
    "                          scaled_series, \n",
    "                          past_covariates, \n",
    "                          future_covariates,\n",
    "                          forecast_horizon, \n",
    "                          start_split,\n",
    "                          verbose=False) -> List:\n",
    "    backtests = [model.historical_forecasts(series=serie,\n",
    "                                            #past_covariates=past_cov,\n",
    "                                            future_covariates=future_cov,\n",
    "                                            forecast_horizon=forecast_horizon,\n",
    "                                            start=start_split,\n",
    "                                            stride=1,\n",
    "                                            retrain=False,\n",
    "                                            last_points_only=True,\n",
    "                                            num_samples=100,\n",
    "                                            verbose=verbose)\n",
    "                 for serie, past_cov, future_cov in list(zip(scaled_series, \n",
    "                                                             past_covariates, \n",
    "                                                             future_covariates))]\n",
    "    return backtests\n",
    "\n",
    "def fit_local_models(models, \n",
    "                     scaled_series, \n",
    "                     past_covariates, \n",
    "                     future_covariates) -> List:\n",
    "    for model, serie, past_cov, future_cov in list(zip(models, \n",
    "                                                       scaled_series, \n",
    "                                                       past_covariates, \n",
    "                                                       future_covariates)):\n",
    "        model.fit(series=serie[0], \n",
    "                  past_covariates=past_cov[0],\n",
    "                  future_covariates=future_cov[0],\n",
    "                  verbose=True\n",
    "             )\n",
    "    return models\n",
    "\n",
    "def fit_global_model(model, \n",
    "                     scaled_series, \n",
    "                     past_covariates, \n",
    "                     future_covariates) -> List:\n",
    "    for serie, past_cov, future_cov in list(zip(scaled_series, \n",
    "                                                past_covariates, \n",
    "                                                future_covariates)):\n",
    "        model.fit(series=serie[0], \n",
    "                  #past_covariates=past_cov[0],\n",
    "                  future_covariates=future_cov[0],\n",
    "                  verbose=True\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "def calculate_loss(scalers, \n",
    "                   splited_series, \n",
    "                   backtests) -> float:\n",
    "    rmse_losses = list()\n",
    "    mape_losses = list()\n",
    "    for scaler, serie_list, backtest in list(zip(scalers, splited_series, backtests)):\n",
    "        \n",
    "        val_serie = scaler.inverse_transform(serie_list[1])\n",
    "        backtest = scaler.inverse_transform(backtest)\n",
    "\n",
    "        val_serie = val_serie.map(lambda x: (np.exp(x) - 1))\n",
    "        backtest = backtest.map(lambda x: (np.exp(x) - 1))\n",
    "\n",
    "        rmse_losses.append(\n",
    "            rmse(val_serie.slice_intersect(backtest), backtest)\n",
    "        )\n",
    "        mape_losses.append(\n",
    "            mape(val_serie.slice_intersect(backtest), backtest)\n",
    "        )\n",
    "    mean_rmse, std_rmse = np.mean(rmse_losses), np.std(rmse_losses)\n",
    "    mean_mape, std_mape = np.mean(mape_losses), np.std(mape_losses)\n",
    "    print(f\"rmse_mean = {mean_rmse}, rmse_std = {std_rmse}\")\n",
    "    print(f\"mape_mean = {mean_mape}, mape_std = {std_mape}\")\n",
    "    return mean_rmse\n",
    "    \n",
    "\n",
    "def plot_backtest_forecasts(scalers, splited_series, backtests) -> None:\n",
    "    for scaler, serie_list, backtest in list(zip(scalers, splited_series, backtests)):\n",
    "        val_serie = serie_list[1]\n",
    "\n",
    "        val_serie = scaler.inverse_transform(val_serie)\n",
    "        backtest = scaler.inverse_transform(backtest)\n",
    "\n",
    "        val_serie = val_serie.map(lambda x: (np.exp(x) - 1))\n",
    "        backtest = backtest.map(lambda x: (np.exp(x) - 1))\n",
    "\n",
    "        val_serie.slice_intersect(backtest).plot(label='data')\n",
    "        backtest.plot(lw=2, label='forecast')\n",
    "        #covs.slice_intersect(backtest)[:slice_size].plot(label='covariates')\n",
    "        error = print_error_metrics(val_serie.slice_intersect(backtest).values(), backtest.values())\n",
    "\n",
    "        plt.title(f'MAPE: {mape(val_serie,backtest)}, RMSE: {rmse(val_serie, backtest)}')\n",
    "        plt.title(error)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99cdbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import TFTModel, RNNModel\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "\n",
    "model = TFTModel(\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        output_chunk_length=forecast_horizon,\n",
    "        hidden_size=64,\n",
    "        lstm_layers=1,\n",
    "        num_attention_heads=4,\n",
    "        dropout=0.1,\n",
    "        batch_size=256,\n",
    "        n_epochs=10,\n",
    "        add_relative_index=False,\n",
    "        add_encoders=None,\n",
    "        likelihood=QuantileRegression(\n",
    "            quantiles=quantiles\n",
    "        ),\n",
    "        # loss_fn=MSELoss(),\n",
    "        random_state=42,\n",
    ")\n",
    "\n",
    "model_nbeats = NBEATSModel(\n",
    "    input_chunk_length=input_chunk_length,\n",
    "    output_chunk_length=forecast_horizon,\n",
    "    generic_architecture=True,\n",
    "    num_stacks=10,\n",
    "    num_blocks=1,\n",
    "    num_layers=4,\n",
    "    layer_widths=512,\n",
    "    n_epochs=10,\n",
    "    nr_epochs_val_period=1,\n",
    "    batch_size=256,\n",
    "    model_name=\"nbeats_run\",\n",
    "    likelihood=QuantileRegression(quantiles=quantiles),\n",
    ")\n",
    "\n",
    "model = RNNModel(input_chunk_length=38, \n",
    "                  output_chunk_length=19, \n",
    "                  n_rnn_layers=2, \n",
    "                  model=\"LSTM\",\n",
    "                  hidden_dim=40,\n",
    "                  dropout=0.1,\n",
    "                  batch_size=1056,\n",
    "                  n_epochs=100,\n",
    "                  optimizer_kwargs={\"lr\": 1e-3},\n",
    "                  random_state=0,\n",
    "                  likelihood=QuantileRegression(quantiles=quantiles),\n",
    "                 )\n",
    "\n",
    "model = fit_global_model(model, splited_series, splited_past_covariates, splited_future_covariates)\n",
    "backtests = backtest_global_model(model, scaled_series, past_covariates, \n",
    "                                  future_covariates, forecast_horizon, start_split=start_split, verbose=True)\n",
    "\n",
    "loss = calculate_loss(scalers, splited_series, backtests)\n",
    "plot_backtest_forecasts(scalers, splited_series, backtests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e53b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "backtests = backtest_global_model(model, scaled_series, past_covariates, \n",
    "                                  future_covariates, forecast_horizon, start_split=start_split, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c0071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_backtest_forecasts(scalers, splited_series, backtests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68841e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from darts.models.forecasting.regression_ensemble_model import RegressionEnsembleModel\n",
    "from darts.utils.utils import ModelMode, SeasonalityMode\n",
    "from darts.models import Theta, RegressionModel, ExponentialSmoothing\n",
    "\n",
    "stat_models = [RegressionEnsembleModel(\n",
    "                                forecasting_models=[\n",
    "                                                    ExponentialSmoothing(trend=ModelMode.ADDITIVE, \n",
    "                                                                         seasonal=SeasonalityMode.NONE,\n",
    "                                                                         seasonal_periods=7,\n",
    "                                                                        ), \n",
    "                                                    Theta(theta=2, \n",
    "                                                          seasonality_period=7, \n",
    "                                                          season_mode=SeasonalityMode.ADDITIVE\n",
    "                                                    )\n",
    "                                                   ], \n",
    "                                regression_train_n_points=int(len(scaled_series[0])*0.5*(1-start_split)),\n",
    "                                regression_model=KernelRidge()\n",
    ") \n",
    "                for model in range(len(scaled_series))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212fe4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR, LinearSVR, NuSVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "class Objective(object):\n",
    "    def __init__(self, scaled_series, past_covariates, future_covariates,\n",
    "                 splited_series, splited_past_covariates, \n",
    "                 splited_future_covariates, forecast_horizon, scalers):\n",
    "        # Hold this implementation specific arguments as the fields of the class.\n",
    "        self.scaled_series = scaled_series\n",
    "        self.past_covariates = past_covariates\n",
    "        self.future_covariates = future_covariates\n",
    "        self.splited_series = splited_series\n",
    "        self.splited_past_covariates = splited_past_covariates \n",
    "        self.splited_future_covariates = splited_future_covariates\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.scalers = scalers\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        # Calculate an objective value by using the extra arguments.\n",
    "\n",
    "        regressor_type = trial.suggest_categorical(\"regressor\", [\"SVC\", \"LinearSVR\", \"NuSVR\"])\n",
    "        #gammas = trial.suggest_categorical(\"regressor\", ['scale', 'auto'])\n",
    "        #kernel = trial.suggest_categorical(\"kernel\", [\"rbf\", \"linear\", \"poly\"])\n",
    "        svc_c = trial.suggest_float(\"svc_c\", 1e-3, 1e2, log=True)\n",
    "        svc_epsilon = trial.suggest_float(\"svc_epsilon\", 1e-1, 1e1, log=True)\n",
    "\n",
    "        if regressor_type == \"SVC\":\n",
    "            model = SVR(kernel=\"poly\", C=svc_c, gamma=\"auto\", \n",
    "                        degree=3, epsilon=svc_epsilon, coef0=1, random_state=nrd)\n",
    "            #svr_rbf = SVR(kernel=\"rbf\", C=100, gamma=0.1, epsilon=0.1)\n",
    "            #svr_lin = SVR(kernel=\"linear\", C=100, gamma=\"auto\")\n",
    "            #model = SVR(kernel=\"poly\", C=100, gamma=\"auto\", degree=3, epsilon=0.1, coef0=1)\n",
    "            \n",
    "        elif regressor_type == \"LinearSVR\":\n",
    "            model = LinearSVR(C=svc_c, gamma=\"auto\", degree=3, epsilon=svc_epsilon, coef0=1, random_state=nrd)\n",
    "        \n",
    "        elif regressor_type == \"NuSVR\":\n",
    "            svr_nu = trial.suggest_float(\"svr_nu\", 1e-1, 1e0, log=True)\n",
    "            model = NuSVR(nu=svr_nu, kernel=\"poly\", C=svc_c, gamma=\"auto\", \n",
    "                          degree=3, epsilon=svc_epsilon, coef0=1, random_state=nrd)\n",
    "\n",
    "        \n",
    "        models = fit_local_models(models, self.splited_series, self.splited_past_covariates, self.splited_future_covariates)\n",
    "        backtests = backtest_local_models(models, self.scaled_series, self.past_covariates, \n",
    "                                          self.future_covariates, self.forecast_horizon, \n",
    "                                          start_split=0.9)\n",
    "        \n",
    "        loss = calculate_loss(self.scalers, self.splited_series, backtests)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38000caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models.forecasting.gradient_boosted_model import LightGBMModel\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839fff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.linear_model import (RidgeCV, \n",
    "                                  #TweedieRegressor, \n",
    "                                  #SGDRegressor, \n",
    "                                  LassoCV, \n",
    "                                  HuberRegressor, \n",
    "                                  ElasticNetCV,\n",
    "                                  BayesianRidge,\n",
    "                                  HuberRegressor,\n",
    "                                 )\n",
    "SEED = 42\n",
    "nrd = np.random.seed(SEED)\n",
    "\n",
    "# Turn off optuna log notes.\n",
    "optuna.logging.set_verbosity(optuna.logging.WARN)\n",
    "\n",
    "def logging_callback(study, frozen_trial):\n",
    "    previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n",
    "    if previous_best_value != study.best_value:\n",
    "        study.set_user_attr(\"previous_best_value\", study.best_value)\n",
    "        print(\n",
    "            \"Trial {} finished with best value: {} and parameters: {}. \".format(\n",
    "            frozen_trial.number,\n",
    "            frozen_trial.value,\n",
    "            frozen_trial.params,\n",
    "            )\n",
    "        )\n",
    "\n",
    "class Objective(object):\n",
    "    def __init__(self, scaled_series, past_covariates, future_covariates,\n",
    "                 splited_series, splited_past_covariates, \n",
    "                 splited_future_covariates, forecast_horizon, scalers,\n",
    "                 lags_future_1=7, lags_future_2=2, lags=14, lags_past=14):\n",
    "        # Hold this implementation specific arguments as the fields of the class.\n",
    "        self.scaled_series = scaled_series\n",
    "        self.past_covariates = past_covariates\n",
    "        self.future_covariates = future_covariates\n",
    "        self.splited_series = splited_series\n",
    "        self.splited_past_covariates = splited_past_covariates \n",
    "        self.splited_future_covariates = splited_future_covariates\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.scalers = scalers\n",
    "        self.lags_future_1 = lags_future_1\n",
    "        self.lags_future_2 = lags_future_2\n",
    "        self.lags = lags\n",
    "        self.lags_past = lags_past\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        # Calculate an objective value by using the extra arguments.\n",
    "\n",
    "        #alpha = trial.suggest_float(\"alpha\", 1e-3, 1e0, log=True)\n",
    "        #l1_ratio = trial.suggest_categorical(\"l1_ratio\", [.1, .25, .5, .7, .9, .95, .99, 1])\n",
    "        l1_ratio = trial.suggest_float(\"l1_ratio\", 1e-1, 1e0, log=True)\n",
    "        #lags_past = trial.suggest_int(\"lags_past\", 2, 14, log=True)\n",
    "        #lags = trial.suggest_int(\"lags\", 2, 14, log=True)\n",
    "        #lags_future_1 = trial.suggest_int(\"lags_future_1\", 1, 7, log=True)\n",
    "        #lags_future_2 = trial.suggest_int(\"lags_future_2\", 0, 5, log=False, step=1)\n",
    "        \n",
    "        # init the models \n",
    "        models = [RegressionModel(lags=self.lags, \n",
    "                                  lags_past_covariates=self.lags_past,\n",
    "                                  lags_future_covariates=(self.lags_future_1, self.lags_future_2),\n",
    "                                  model=ElasticNetCV(#alphas=alphas,\n",
    "                                                     l1_ratio=l1_ratio,\n",
    "                                                     random_state=nrd))\n",
    "                  for model in range(len(self.scaled_series))]\n",
    "\n",
    "        models = fit_local_models(models, self.splited_series, self.splited_past_covariates, self.splited_future_covariates)\n",
    "        backtests = backtest_local_models(models, self.scaled_series, self.past_covariates, \n",
    "                                          self.future_covariates, self.forecast_horizon, \n",
    "                                          start_split=0.9)\n",
    "        \n",
    "        loss = calculate_loss(self.scalers, self.splited_series, backtests)\n",
    "        return loss\n",
    "\n",
    "study_name = \"ElasticNetCV\"\n",
    "study = optuna.create_study(direction=\"minimize\",\n",
    "                            sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "                            #storage=\"sqlite:///example.db\",\n",
    "                            study_name=study_name\n",
    "                           )\n",
    "objective = Objective(scaled_series=scaled_series,\n",
    "                      past_covariates=past_covariates,\n",
    "                      future_covariates=future_covariates,\n",
    "                      splited_series=splited_series,\n",
    "                      splited_past_covariates=splited_past_covariates,\n",
    "                      splited_future_covariates=splited_future_covariates,\n",
    "                      forecast_horizon=forecast_horizon,\n",
    "                      scalers=scalers\n",
    ")\n",
    "study.optimize(objective, n_trials=100, callbacks=[logging_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best trial: \\n{study.best_trial}\\n\")\n",
    "print(f\"Best value: {study.best_value}\\n\")\n",
    "print(f\"Best params: {study.best_params}\\n\")\n",
    "# print(study.trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_intermediate_values\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_slice\n",
    "\n",
    "plot_contour(study)\n",
    "# plot_intermediate_values(study)\n",
    "fig = plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"ElasticNetCV\"\n",
    "results_directory = \"./results\"\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "study.trials_dataframe().to_csv(f\"{results_directory}/{study_name}.csv\", index=False)\n",
    "plot_parallel_coordinate(study).write_html(f\"{results_directory}/{study_name}_parallel.html\")\n",
    "plot_contour(study).write_html(f\"{results_directory}/{study_name}_contour.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8e8663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ee934e0",
   "metadata": {},
   "source": [
    "### Model per series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from darts.models.forecasting.gradient_boosted_model import LightGBMModel\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, StackingRegressor\n",
    "from darts.models.forecasting.regression_model import RegressionModel\n",
    "\n",
    "from sklearn.linear_model import (RidgeCV, \n",
    "                                  #TweedieRegressor, \n",
    "                                  #SGDRegressor, \n",
    "                                  LassoCV, \n",
    "                                  HuberRegressor, \n",
    "                                  ElasticNetCV,\n",
    "                                  #BayesianRidge,\n",
    "                                 )\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR, LinearSVR, NuSVR\n",
    "\n",
    "# init the models \n",
    "models = [RegressionModel(lags=14, \n",
    "                          lags_past_covariates=14,\n",
    "                          lags_future_covariates=(7,2),\n",
    "                          model=LGBMRegressor(),\n",
    "                          output_chunk_length=forecast_horizon,\n",
    "                         )\n",
    "          for model in range(len(scaled_series))]\n",
    "\n",
    "model = RegressionModel(lags=14, \n",
    "                        lags_past_covariates=14,\n",
    "                        lags_future_covariates=(7,2),\n",
    "                        model=ElasticNetCV(l1_ratio=0.4),\n",
    "                        output_chunk_length=forecast_horizon,\n",
    "                        )\n",
    "\n",
    "\n",
    "# models = [RegressionEnsembleModel(\n",
    "#                                 forecasting_models=[RegressionModel(lags=14, \n",
    "#                                                                     lags_past_covariates=14,\n",
    "#                                                                     lags_future_covariates=(7,2),\n",
    "#                                                                     model=RidgeCV())\n",
    "#                                                    #, LassoCV(), HuberRegressor()\n",
    "#                                                    ], \n",
    "#                                 regression_train_n_points=int(len(scaled_series[0])*0.5*(1-start_split)),\n",
    "#                                 regression_model=ElasticNet()\n",
    "#                 ) \n",
    "#                 for model in range(len(scaled_series))\n",
    "# ]\n",
    "# estimators = [\n",
    "#     ('lr', RidgeCV()),\n",
    "#     ('svr', LassoCV()),\n",
    "#     ('huber', HuberRegressor())\n",
    "# ]\n",
    "\n",
    "# models = [RegressionModel(lags=14, \n",
    "#                           lags_past_covariates=14,\n",
    "#                           lags_future_covariates=(7,2),\n",
    "#                           model=StackingRegressor(\n",
    "#                                               estimators=estimators,\n",
    "#                                               final_estimator=RandomForestRegressor(n_estimators=10)\n",
    "#                             )\n",
    "#                         )\n",
    "#            for model in range(len(scaled_series))\n",
    "# ]\n",
    "\n",
    "local = True\n",
    "\n",
    "if local:\n",
    "    models = fit_local_models(models, splited_series, splited_past_covariates, splited_future_covariates)\n",
    "    backtests = backtest_local_models(models, scaled_series, past_covariates, \n",
    "                                  future_covariates, forecast_horizon, start_split=0.9, verbose=True)\n",
    "else:\n",
    "    model = fit_global_model(model, splited_series, splited_past_covariates, splited_future_covariates)\n",
    "    backtests = backtest_global_model(model, scaled_series, past_covariates, \n",
    "                                      future_covariates, forecast_horizon, start_split=0.9, verbose=True)\n",
    "\n",
    "loss = calculate_loss(scalers, splited_series, backtests)\n",
    "plot_backtest_forecasts(scalers, splited_series, backtests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423e6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24bad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6269560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import RNNModel\n",
    "from darts.models import TFTModel\n",
    "from darts.models import TCNModel\n",
    "from darts.models import TransformerModel\n",
    "from darts.models import BlockRNNModel\n",
    "\n",
    "from darts.utils.likelihood_models import (\n",
    "    GaussianLikelihood,\n",
    "    PoissonLikelihood,\n",
    "    NegativeBinomialLikelihood,\n",
    "    BernoulliLikelihood,\n",
    "    GammaLikelihood,\n",
    "    GumbelLikelihood,\n",
    "    LaplaceLikelihood,\n",
    "    BetaLikelihood,\n",
    "    ExponentialLikelihood,\n",
    "    DirichletLikelihood,\n",
    "    GeometricLikelihood,\n",
    "    CauchyLikelihood,\n",
    "    ContinuousBernoulliLikelihood,\n",
    "    HalfNormalLikelihood,\n",
    "    LogNormalLikelihood,\n",
    "    WeibullLikelihood,\n",
    "    QuantileRegression,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "brnn_no_cov = BlockRNNModel(input_chunk_length=input_chunk_length,\n",
    "                            output_chunk_length=forecast_horizon,\n",
    "                            n_rnn_layers=2,\n",
    "                            likelihood=QuantileRegression(quantiles=quantiles),\n",
    "                            random_state=42)\n",
    "\n",
    "deepar = RNNModel(input_chunk_length=38, \n",
    "                  output_chunk_length=19, \n",
    "                  n_rnn_layers=2, \n",
    "                  model=\"LSTM\",\n",
    "                  #hidden_dim=20,\n",
    "                  dropout=0.1,\n",
    "                  batch_size=256,\n",
    "                  n_epochs=10,\n",
    "                  optimizer_kwargs={\"lr\": 1e-3},\n",
    "                  random_state=0,\n",
    "                  likelihood=BetaLikelihood()#QuantileRegression(quantiles=quantiles),\n",
    "                 )\n",
    "\n",
    "deepar = TFTModel(\n",
    "        input_chunk_length=input_chunk_length,\n",
    "        output_chunk_length=forecast_horizon,\n",
    "        hidden_size=64,\n",
    "        lstm_layers=1,\n",
    "        num_attention_heads=4,\n",
    "        dropout=0.1,\n",
    "        batch_size=256,\n",
    "        n_epochs=10,\n",
    "        add_relative_index=False,\n",
    "        add_encoders=None,\n",
    "        likelihood=QuantileRegression(\n",
    "            quantiles=quantiles\n",
    "        ),  # QuantileRegression is set per default\n",
    "        # loss_fn=MSELoss(),\n",
    "        random_state=42,\n",
    ")\n",
    "\n",
    "deeptcn = TCNModel(\n",
    "    input_chunk_length=input_chunk_length,\n",
    "    output_chunk_length=forecast_horizon,\n",
    "    batch_size=256,\n",
    "    kernel_size=2,\n",
    "    num_filters=4,\n",
    "    dilation_base=2,\n",
    "    dropout=0.1,\n",
    "    random_state=0,\n",
    "    likelihood=QuantileRegression(\n",
    "            quantiles=quantiles\n",
    "        )\n",
    ")\n",
    "\n",
    "trans_model = TransformerModel(\n",
    "    input_chunk_length=input_chunk_length,\n",
    "    output_chunk_length=forecast_horizon,\n",
    "    batch_size=256,\n",
    "    n_epochs=10,\n",
    "    model_name=\"transformer\",\n",
    "    nr_epochs_val_period=1,\n",
    "    d_model=16,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    dim_feedforward=128,\n",
    "    dropout=0.1,\n",
    "    activation=\"relu\",\n",
    "    random_state=42,\n",
    "    save_checkpoints=True,\n",
    "    force_reset=True,\n",
    ")\n",
    "\n",
    "\n",
    "from darts.models import NBEATSModel\n",
    "\n",
    "encoders = {\n",
    "    \"cyclic\": {\"future\": [\"month\",\"day\"]},\n",
    "    \"datetime_attribute\": {\"future\": [\"dayofweek\", \"day\"]},\n",
    "    \"position\": {\"future\": [\"relative\"]},\n",
    "    #\"position\": {\"past\": [\"absolute\"], \"future\": [\"relative\"]},\n",
    "    #\"custom\": {\"past\": [lambda idx: (idx.day - 1950) / 50]},\n",
    "    \"transformer\": Scaler(),\n",
    "}\n",
    "\n",
    "\n",
    "model_nbeats = NBEATSModel(\n",
    "    input_chunk_length=input_chunk_length,\n",
    "    output_chunk_length=forecast_horizon,\n",
    "    generic_architecture=True,\n",
    "    num_stacks=10,\n",
    "    num_blocks=1,\n",
    "    num_layers=4,\n",
    "    layer_widths=512,\n",
    "    n_epochs=10,\n",
    "    nr_epochs_val_period=1,\n",
    "    batch_size=256,\n",
    "    model_name=\"nbeats_run\",\n",
    "    likelihood=QuantileRegression(quantiles=quantiles),\n",
    ")\n",
    "\n",
    "for serie, past_cov, future_cov in list(zip(splited_series, \n",
    "                                            splited_past_covariates, \n",
    "                                            splited_future_covariates)):\n",
    "    model_nbeats.fit(series=serie[0], \n",
    "                     past_covariates=past_cov[0],\n",
    "                     val_series=serie[1].drop_after(0.5),\n",
    "                     val_past_covariates=past_cov[1].drop_after(0.5),\n",
    "                     verbose=True)\n",
    "\n",
    "for serie, past_cov, future_cov in list(zip(splited_series, \n",
    "                                            splited_past_covariates, \n",
    "                                            splited_future_covariates)):\n",
    "    deepar.fit(series=serie[0], \n",
    "               #past_covariates=past_cov[0],\n",
    "               future_covariates=future_cov[0],\n",
    "               val_series=serie[1].drop_after(0.5),\n",
    "               #val_past_covariates=past_cov[1].drop_after(0.5),\n",
    "               val_future_covariates=future_cov[1].drop_after(0.5),\n",
    "               verbose=True,\n",
    "               #epochs=10\n",
    "              )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ca6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7848864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32517806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "def4175e",
   "metadata": {},
   "source": [
    "### Naive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed897479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import NaiveEnsembleModel\n",
    "from darts.models import NaiveSeasonal\n",
    "from darts.models import NaiveDrift\n",
    "from sklearn.linear_model import RidgeCV, TweedieRegressor, ElasticNet, BayesianRidge, LassoCV\n",
    "# TweedieRegressor(power=2, alphas = np.logspace(-6, 6, 25))\n",
    "from darts.models.forecasting.linear_regression_model import LinearRegressionModel\n",
    "\n",
    "\n",
    "naive_models = [RegressionEnsembleModel(\n",
    "                                forecasting_models=[NaiveDrift(), NaiveSeasonal(14), NaiveSeasonal(7)], \n",
    "                                regression_train_n_points=int(len(scaled_series[0])*0.5*(1-start_split)),\n",
    "                                regression_model=ElasticNet()\n",
    "                ) \n",
    "                for model in range(len(scaled_series))\n",
    "]\n",
    "\n",
    "for model, serie, past_cov, future_cov in list(zip(naive_models, \n",
    "                                                   splited_series, \n",
    "                                                   splited_past_covariates, \n",
    "                                                   splited_future_covariates)):\n",
    "    model.fit(series=serie[0], \n",
    "              #past_covariates=past_cov[0],\n",
    "              future_covariates=future_cov[0],\n",
    "              #verbose=True\n",
    "         )\n",
    "\n",
    "backtests = [model.historical_forecasts(series=serie,\n",
    "                                        start=start_split+0.5*(1-start_split),\n",
    "                                        #past_covariates=past_cov,\n",
    "                                        future_covariates=future_cov,\n",
    "                                        forecast_horizon=forecast_horizon,\n",
    "                                        stride=1,\n",
    "                                        retrain=True,\n",
    "                                        last_points_only=True,\n",
    "                                        verbose=True)\n",
    "             \n",
    "             for model, serie, past_cov, future_cov in list(zip(naive_models, \n",
    "                                                                scaled_series, \n",
    "                                                                past_covariates, \n",
    "                                                                future_covariates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e63fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_error_metrics\n",
    "\n",
    "calculate_loss(scalers, splited_series, backtests)\n",
    "\n",
    "for scaler, serie_list, backtest, covs in list(zip(scalers, splited_series, backtests, past_covariates)):\n",
    "    val_serie = serie_list[1]\n",
    "    \n",
    "    val_serie = scaler.inverse_transform(val_serie)\n",
    "    backtest = scaler.inverse_transform(backtest)\n",
    "    \n",
    "    val_serie = val_serie.map(lambda x: (np.exp(x) - 1))\n",
    "    backtest = backtest.map(lambda x: (np.exp(x) - 1))\n",
    "    \n",
    "    slice_size = 1000\n",
    "    val_serie.slice_intersect(backtest)[:slice_size].plot(label='data')\n",
    "    backtest[:slice_size].plot(lw=2, label='forecast')\n",
    "    #covs.slice_intersect(backtest)[:slice_size].plot(label='covariates')\n",
    "    error = print_error_metrics(val_serie.slice_intersect(backtest).values(), backtest.values())\n",
    "    \n",
    "    plt.title(f'MAPE: {mape(val_serie,backtest)}, RMSE: {rmse(val_serie, backtest)}')\n",
    "    plt.title(error)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# print(\n",
    "#     f\"Mean Absolute Error:     {mae.mean():.3f} +/- {mae.std():.3f}\\n\"\n",
    "#     f\"Root Mean Squared Error: {rmse.mean():.3f} +/- {rmse.std():.3f}\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd97f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d3fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65e7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22269cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf2ed12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb53cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743f0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80719b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtest the models \n",
    "from darts.utils.statistics import plot_hist\n",
    "\n",
    "for serie, past_cov, future_cov in list(zip(scaled_series, past_covariates, future_covs)):\n",
    "    raw_errors = model.backtest(\n",
    "        series=serie,\n",
    "        past_covariates=past_cov,\n",
    "        future_covariates=future_cov,\n",
    "        start=start_split,\n",
    "        forecast_horizon=forecast_horizon,\n",
    "        stride=1,\n",
    "        retrain=False,\n",
    "        last_points_only=True,\n",
    "        #metric=mape, \n",
    "        reduction=None, \n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    plot_hist(\n",
    "        raw_errors,\n",
    "        bins=np.arange(0, max(raw_errors), 1),\n",
    "        title=\"Individual backtest error scores (histogram)\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd141cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00660b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6127e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    model.fit(train)\n",
    "    forecast = model.predict(len(val))\n",
    "    print(\"model {} obtains MAPE: {:.2f}%\".format(model, mape(val, forecast)))\n",
    "    \n",
    "from darts.utils.statistics import plot_residuals_analysis, plot_hist, display_forecast\n",
    "pred_series = model_nbeats.historical_forecasts(\n",
    "    series,\n",
    "    start=pd.Timestamp(\"20170901\"),\n",
    "    forecast_horizon=7,\n",
    "    stride=5,\n",
    "    retrain=False,\n",
    "    verbose=True,\n",
    ")\n",
    "display_forecast(pred_series, series, \"7 day\", start_date=pd.Timestamp(\"20170901\"))\n",
    "\n",
    "\n",
    "plot_residuals_analysis(best_theta_model.residuals(series))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_errors = best_theta_model.backtest(\n",
    "    series, start=0.6, forecast_horizon=3, metric=mape, reduction=None, verbose=True\n",
    ")\n",
    "\n",
    "from darts.utils.statistics import plot_hist\n",
    "\n",
    "plot_hist(\n",
    "    raw_errors,\n",
    "    bins=np.arange(0, max(raw_errors), 1),\n",
    "    title=\"Individual backtest error scores (histogram)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0e429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410cb3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8e0bb0da2aff65736b499a73199d9b3916fe5784b22bc0d777fb56d771df7b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
